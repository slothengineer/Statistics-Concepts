{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb2b6ca-44b3-42d6-b4f6-503dbae44157",
   "metadata": {},
   "source": [
    "###  Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.\n",
    "\n",
    "Analysis of Variance (ANOVA) is a statistical method used to compare means across multiple groups. It helps determine whether there are any significant differences between the means of these groups. ANOVA makes certain assumptions about the data for its results to be valid. Violations of these assumptions can impact the reliability and validity of ANOVA results. The assumptions are:\n",
    "\n",
    "1. **Normality**: The data within each group should be approximately normally distributed. This means that the values within each group should follow a bell-shaped curve. Violation of this assumption can lead to inaccurate p-values and incorrect conclusions.\n",
    "\n",
    "2. **Homogeneity of Variance (Homoscedasticity)**: The variances within each group should be roughly equal. In other words, the spread of data points around the mean should be similar across all groups. If the variances are significantly different, it can affect the reliability of ANOVA results.\n",
    "\n",
    "3. **Independence**: Observations within each group should be independent of each other. This means that the value of one observation should not be influenced by or related to the value of another observation. Violations can lead to increased Type I error rates (false positives) and invalid results.\n",
    "\n",
    "Examples of violations that could impact the validity of ANOVA results:\n",
    "\n",
    "1. **Normality Violation**: Suppose you are comparing the effectiveness of three different teaching methods on student performance. If the scores within each teaching method group are not normally distributed, ANOVA assumptions are violated. For example, if one group's scores are heavily skewed or have extreme outliers, this could impact the validity of ANOVA results.\n",
    "\n",
    "2. **Homoscedasticity Violation**: Imagine you are comparing the yields of three different fertilizer types on crop production. If the variances of crop yields within each fertilizer group are significantly different, ANOVA assumptions are violated. Unequal variances might make it difficult to discern whether differences in means are due to the fertilizers or simply due to different variances.\n",
    "\n",
    "3. **Independence Violation**: Let's say you're studying the effect of three different exercise routines on weight loss. If participants' weights are measured over time and these measurements are correlated, then the independence assumption is violated. For instance, if the weight loss of one participant in a particular routine is influenced by the weight loss of another participant, the ANOVA results might not accurately represent the treatment effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b99f89-d1b4-4dba-929d-cee4f7d95252",
   "metadata": {},
   "source": [
    "### What are the three types of ANOVA, and in what situations would each be used?\n",
    "\n",
    "The three types of Analysis of Variance (ANOVA) are:\n",
    "\n",
    "1. **One-Way ANOVA**: This type of ANOVA is used when you have one categorical independent variable (factor) with three or more levels, and you want to compare the means of a continuous dependent variable across these levels. It helps determine if there are any significant differences between the means of the groups.\n",
    "\n",
    "   **Example**: Suppose you want to compare the average test scores of students from three different schools (A, B, and C) to see if there are any significant differences in performance.\n",
    "\n",
    "2. **Two-Way ANOVA**: Two-Way ANOVA is used when you have two categorical independent variables (factors) and one continuous dependent variable. It allows you to examine the effects of these two factors and their interaction on the dependent variable's mean.\n",
    "\n",
    "   **Example**: Imagine you are studying the effects of both gender and different teaching methods on student performance. You want to know if there are differences in performance due to gender, teaching method, and whether the effect of one factor depends on the other (interaction effect).\n",
    "\n",
    "3. **Three-Way ANOVA**: This type of ANOVA involves three categorical independent variables and one continuous dependent variable. It allows you to examine the effects of three factors and their interactions on the dependent variable.\n",
    "\n",
    "   **Example**: Let's say you're studying the impact of three factors—temperature (low, medium, high), humidity (low, medium, high), and lighting (natural, artificial)—on plant growth. You want to know how these factors individually and collectively influence the growth rate of the plants.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **One-Way ANOVA** compares means across multiple levels of a single factor.\n",
    "- **Two-Way ANOVA** assesses the main effects and interaction between two factors.\n",
    "- **Three-Way ANOVA** examines the main effects and interaction among three factors.\n",
    "\n",
    "Choosing the appropriate type of ANOVA depends on the number of factors you are studying and their relationships. It's important to ensure that the assumptions of ANOVA are met for the results to be valid. If these assumptions are violated or if the data does not meet the ANOVA requirements, alternative statistical methods or transformations might be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6f81f-d48c-4d61-a092-43bfcb3341af",
   "metadata": {},
   "source": [
    "### What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "\n",
    "The partitioning of variance in ANOVA refers to the process of decomposing the total variability observed in a dataset into different components that can be attributed to various sources. This decomposition helps us understand how much of the total variability is explained by the factors of interest and how much is due to random variability. In ANOVA, the total variability in the data is divided into two main components: the variability between groups and the variability within groups. This partitioning is crucial because it forms the basis for calculating the F-statistic and assessing the significance of the factors being studied.\n",
    "\n",
    "The components of variance in ANOVA are:\n",
    "\n",
    "1. **Between-Groups Variance (Treatment Variance)**: This component measures the variation in the means of different groups or categories being compared. It indicates whether there are significant differences in means across these groups. If the between-groups variance is larger than expected due to chance, it suggests that there might be a true effect of the factor being studied.\n",
    "\n",
    "2. **Within-Groups Variance (Error Variance)**: This component captures the random variability within each group. It represents the differences between individual data points and their respective group means. It includes any variability that cannot be attributed to the factors being studied. Larger within-groups variance suggests that data points within each group are more spread out, making it harder to discern meaningful differences.\n",
    "\n",
    "The importance of understanding the partitioning of variance in ANOVA lies in the following aspects:\n",
    "\n",
    "1. **Interpretation of Results**: By understanding how the total variability is divided into between-groups and within-groups variance, researchers can interpret the relative contributions of the factors being studied and the random variability to the overall variation in the data.\n",
    "\n",
    "2. **Calculation of F-Statistic**: The F-statistic, which is the ratio of between-groups variance to within-groups variance, is central to ANOVA. It helps determine whether the observed differences between group means are statistically significant. An understanding of the variance components is necessary for calculating and interpreting the F-statistic correctly.\n",
    "\n",
    "3. **Assessment of Significance**: ANOVA uses the partitioning of variance to assess whether the differences between group means are larger than expected by chance. This is done by comparing the magnitude of the between-groups variance to the within-groups variance and calculating a p-value. If the between-groups variance is significantly larger, it suggests that the factor being studied has a significant effect.\n",
    "\n",
    "4. **Guiding Further Analysis**: The partitioning of variance can guide researchers on where to focus further investigations. If the between-groups variance is small relative to the within-groups variance, it might indicate that the factor being studied does not have a strong effect, and other factors might need to be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d614d-d054-4726-acc2-6b5074ec2817",
   "metadata": {},
   "source": [
    "### How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821bbbd9-448a-4716-9318-ce6b9afacae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 1044.9333333333332\n",
      "Explained Sum of Squares (SSE): 989.7333333333337\n",
      "Residual Sum of Squares (SSR): 55.19999999999948\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "group1 = np.array([10, 12, 14, 15, 11])\n",
    "group2 = np.array([20, 18, 22, 24, 19])\n",
    "group3 = np.array([30, 32, 35, 31, 33])\n",
    "\n",
    "data = np.concatenate((group1, group2, group3))\n",
    "\n",
    "overall_mean = np.mean(data)\n",
    "\n",
    "# total sum of squares (SST)\n",
    "sst = np.sum((data - overall_mean)**2)\n",
    "\n",
    "# group means\n",
    "group_means = np.array([np.mean(group1), np.mean(group2), np.mean(group3)])\n",
    "\n",
    "# explained sum of squares (SSE)\n",
    "sse = np.sum((group_means - overall_mean)**2 * len(group1))\n",
    "\n",
    "# residual sum of squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b406142-6aff-4125-bda5-3d6e3e89e47d",
   "metadata": {},
   "source": [
    "### In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6519da-2b16-4611-b91b-676015c0a445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              sum_sq    df           F        PR(>F)\n",
      "A          85.716841   2.0   41.267776  1.367404e-13\n",
      "B         187.129571   1.0  180.184456  1.419222e-23\n",
      "A:B         0.409790   2.0    0.197291  8.212911e-01\n",
      "Residual   97.623181  94.0         NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 100\n",
    "A = np.random.choice(['A1', 'A2', 'A3'], size=n)\n",
    "B = np.random.choice(['B1', 'B2'], size=n)\n",
    "Y = 5 + 2 * (A == 'A2') + 3 * (B == 'B2') + np.random.normal(0, 1, n)\n",
    "\n",
    "data = pd.DataFrame({'A': A, 'B': B, 'Y': Y})\n",
    "\n",
    "# two-way ANOVA\n",
    "formula = 'Y ~ A + B + A:B'\n",
    "model = ols(formula, data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4505e17c-a781-4370-b24d-5032db65df3e",
   "metadata": {},
   "source": [
    "### Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?\n",
    "\n",
    "In a one-way ANOVA, the F-statistic and its associated p-value are used to assess whether there are significant differences between the means of the groups being compared. Let's interpret the results we've provided:\n",
    "\n",
    "1. **F-Statistic**: The F-statistic is a ratio of the variance between group means to the variance within groups. In our case, the F-statistic is 5.23.\n",
    "\n",
    "2. **P-Value**: The p-value associated with the F-statistic indicates the probability of observing such an extreme F-statistic value (or more extreme) if there were no real differences between group means. In our case, the p-value is 0.02.\n",
    "\n",
    "Here's how you can interpret these results:\n",
    "\n",
    "1. **Significance of F-Statistic**:\n",
    "   The F-statistic of 5.23 suggests that there are some differences between the means of the groups being compared. However, the F-statistic itself doesn't tell you whether these differences are statistically significant.\n",
    "\n",
    "2. **Significance of P-Value**:\n",
    "   The p-value of 0.02 indicates the probability of observing such an extreme F-statistic value if the null hypothesis is true (i.e., if there are no real differences between group means). A p-value of 0.02 is less than the commonly chosen significance level of 0.05 (5%). This means that the p-value is smaller than the threshold often used to determine statistical significance.\n",
    "\n",
    "3. **Interpretation**:\n",
    "   Given the small p-value (0.02), we have evidence to reject the null hypothesis. This suggests that there are statistically significant differences between the means of the groups being compared. In other words, the groups are not all the same; at least one group mean is different from the others.\n",
    "\n",
    "In conclusion, we can confidently state that you have found statistically significant differences between the groups based on the results of the one-way ANOVA. However, it's important to keep in mind that the ANOVA result does not specify which specific group mean(s) are different from the others. For that, we might need to perform post hoc tests (such as Tukey's HSD or Bonferroni correction) to determine pairwise differences between groups. Additionally, consider the assumptions of ANOVA, sample sizes, and the context of your study while interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0712fb-17dd-4373-b308-ac3eb0123b63",
   "metadata": {},
   "source": [
    "###  In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?\n",
    "\n",
    "Here are some common methods and their potential consequences:\n",
    "\n",
    "1. **Listwise Deletion (Complete Case Analysis)**:\n",
    "   This involves excluding any participant with missing data from the analysis. While it is straightforward, it can lead to loss of statistical power and potential bias if missing data are not completely random. Additionally, it may not be suitable if the proportion of missing data is high.\n",
    "\n",
    "   **Consequences**:\n",
    "   - Loss of statistical power due to reduced sample size.\n",
    "   - Potential bias if missing data are not completely random.\n",
    "   - Reduced generalizability if participants with missing data differ from those without.\n",
    "\n",
    "2. **Mean Imputation**:\n",
    "   This involves replacing missing values with the mean of the available data for that variable. While it's simple to implement, it can artificially reduce variability and lead to inaccurate estimates of means, standard deviations, and correlations.\n",
    "\n",
    "   **Consequences**:\n",
    "   - Artificially reduced variability, leading to underestimation of standard errors.\n",
    "   - Altered relationships between variables due to distorted correlations.\n",
    "   - Incorrect confidence intervals and p-values.\n",
    "\n",
    "3. **Last Observation Carried Forward (LOCF)**:\n",
    "   This method replaces missing values with the value from the last observed time point for that participant. It can introduce bias, especially if the missing data pattern is related to changes over time.\n",
    "\n",
    "   **Consequences**:\n",
    "   - Potential bias if the last observation is not representative of the participant's true state.\n",
    "   - Distorted patterns in longitudinal data.\n",
    "\n",
    "4. **Linear Interpolation**:\n",
    "   In time-series data, linear interpolation estimates missing values based on neighboring data points. It assumes a linear relationship between time points, which may not always be accurate.\n",
    "\n",
    "   **Consequences**:\n",
    "   - Assumption of linear relationships may not hold.\n",
    "   - May introduce unrealistic values between time points.\n",
    "\n",
    "5. **Multiple Imputation**:\n",
    "   Multiple imputation involves creating several plausible values for each missing observation based on the observed data's distribution. Analyses are then performed on each imputed dataset, and results are combined using specific rules. This method can provide more accurate estimates if done properly.\n",
    "\n",
    "   **Consequences**:\n",
    "   - Requires advanced statistical techniques.\n",
    "   - If not done correctly, can lead to incorrect results.\n",
    "\n",
    "6. **Mixed Effects Models**:\n",
    "   If the missing data mechanism is considered missing at random (MAR), mixed effects models can accommodate missing data by using all available data while accounting for correlations within subjects. This approach requires careful model specification.\n",
    "\n",
    "   **Consequences**:\n",
    "   - Requires understanding of mixed effects models.\n",
    "   - Appropriate model specification is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72385d0-aa42-4d7c-b7d7-17df4377a29f",
   "metadata": {},
   "source": [
    "###  What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.\n",
    "\n",
    "Post-hoc tests are used after conducting an Analysis of Variance (ANOVA) to further explore and compare group differences when the ANOVA result indicates significant overall differences among groups. These tests help identify which specific group means differ from each other. Some common post-hoc tests include:\n",
    "\n",
    "1. **Tukey's Honestly Significant Difference (HSD)**:\n",
    "   Tukey's HSD is widely used for pairwise comparisons among all group means. It controls the overall Type I error rate, making it suitable for situations where you have conducted multiple pairwise comparisons. It is more conservative than some other tests, which helps prevent an inflation of the Type I error rate.\n",
    "\n",
    "2. **Bonferroni Correction**:\n",
    "   The Bonferroni correction involves adjusting the significance level (alpha) for each pairwise comparison to maintain a desired family-wise error rate. This method is conservative, and it controls the overall Type I error rate effectively, but it can be overly conservative when there are many pairwise comparisons.\n",
    "\n",
    "3. **Sidak Correction**:\n",
    "   Similar to Bonferroni, the Sidak correction adjusts the significance level for each comparison. However, it offers a less conservative correction compared to Bonferroni, making it more suitable when there are many comparisons.\n",
    "\n",
    "4. **Holm-Bonferroni Method**:\n",
    "   The Holm-Bonferroni method is a step-down procedure that sequentially adjusts the significance levels for comparisons. It provides good control over the family-wise error rate and can be less conservative than Bonferroni.\n",
    "\n",
    "5. **Dunn's Test**:\n",
    "   Dunn's test is a non-parametric post-hoc test that doesn't assume normality. It's based on rank-ordering the data and can be used when the assumptions of ANOVA are not met.\n",
    "\n",
    "6. **Games-Howell Test**:\n",
    "   The Games-Howell test is an alternative to Tukey's HSD when the assumption of homogeneity of variances is violated. It uses an adjusted degrees of freedom approach to account for unequal variances.\n",
    "\n",
    "**Example**:\n",
    "Suppose you are conducting a study to compare the effectiveness of three different medications (A, B, and C) in reducing blood pressure. After conducting a one-way ANOVA, you find a significant difference in mean blood pressure among the medication groups. Now, you want to determine which specific medication pairs show significant differences in means.\n",
    "\n",
    "In this scenario, you would use a post-hoc test like Tukey's HSD to perform pairwise comparisons between all possible pairs of medication groups. This test would help you identify which specific medication(s) lead to significantly different blood pressure levels. This is especially important because ANOVA only tells you that there's a significant difference among groups but doesn't specify which pairs of groups are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded0ed9-861d-4e44-94cf-87fa9160d6e6",
   "metadata": {},
   "source": [
    "### A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541b9853-6fef-45ec-b320-f61d1ea3bbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 142.06115913459553\n",
      "P-Value: 4.515303205609359e-35\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "diet_A = [3.2, 2.8, 4.5, 2.2, 3.7, 2.5, 3.9, 4.1, 2.9, 3.8,\n",
    "          2.7, 4.0, 3.5, 3.6, 3.3, 2.1, 3.4, 3.1, 2.6, 3.0,\n",
    "          4.2, 3.8, 2.4, 3.2, 4.3, 3.6, 2.8, 3.1, 3.7, 2.9,\n",
    "          3.4, 3.5, 2.3, 4.0, 3.6, 2.5, 3.3, 2.7, 3.9, 4.1,\n",
    "          2.2, 3.8, 4.5, 2.1, 3.0, 4.2, 3.7, 2.6, 3.4, 3.1]\n",
    "\n",
    "diet_B = [2.1, 1.9, 3.0, 1.7, 2.6, 1.5, 2.9, 3.1, 2.0, 2.8,\n",
    "          1.6, 3.2, 2.5, 2.6, 2.3, 1.4, 2.7, 2.2, 1.8, 2.0,\n",
    "          3.0, 2.8, 1.5, 2.1, 3.3, 2.6, 1.9, 2.2, 2.6, 2.0,\n",
    "          2.5, 2.6, 1.3, 3.2, 2.8, 1.5, 2.7, 1.6, 2.9, 3.1,\n",
    "          1.7, 2.8, 3.0, 1.4, 2.0, 3.3, 2.6, 1.8, 2.7, 2.2]\n",
    "\n",
    "diet_C = [1.0, 0.8, 1.5, 0.7, 1.6, 0.5, 1.9, 2.1, 1.0, 1.8,\n",
    "          0.6, 2.0, 1.3, 1.4, 1.1, 0.4, 1.7, 1.2, 0.8, 1.0,\n",
    "          1.5, 1.8, 0.5, 1.1, 2.2, 1.4, 0.7, 1.0, 1.4, 0.8,\n",
    "          1.3, 1.4, 0.3, 2.0, 1.8, 0.5, 1.7, 0.6, 1.9, 2.1,\n",
    "          0.7, 1.8, 2.0, 0.4, 0.9, 2.2, 1.6, 0.8, 1.7, 1.2]\n",
    "\n",
    "# one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4d6a8-4876-429f-83fd-7bab7a173061",
   "metadata": {},
   "source": [
    "To interpret the results, we would compare the p-value to a chosen significance level (e.g., 0.05). If the p-value is less than the significance level, we would reject the null hypothesis and conclude that there are significant differences in mean weight loss among the three diets. If the p-value is greater than the significance level, we would fail to reject the null hypothesis, suggesting that there are no significant differences in mean weight loss among the diets. Remember that statistical significance should be interpreted in the context of our research question, study design, and the assumptions of ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcc9c6-4497-4b1f-b8de-b505975bd40f",
   "metadata": {},
   "source": [
    "### A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21bdbe6c-a9f2-4424-97a2-dbfbe18a088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          sum_sq    df          F  \\\n",
      "C(SoftwareProgram)                     40.563725   2.0  30.783576   \n",
      "C(ExperienceLevel)                     11.499315   1.0  17.453527   \n",
      "C(SoftwareProgram):C(ExperienceLevel)   1.503315   2.0   1.140857   \n",
      "Residual                               15.812480  24.0        NaN   \n",
      "\n",
      "                                             PR(>F)  \n",
      "C(SoftwareProgram)                     2.370554e-07  \n",
      "C(ExperienceLevel)                     3.359512e-04  \n",
      "C(SoftwareProgram):C(ExperienceLevel)  3.362719e-01  \n",
      "Residual                                        NaN  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 30\n",
    "software_program = np.random.choice(['A', 'B', 'C'], size=n)\n",
    "experience_level = np.random.choice(['novice', 'experienced'], size=n)\n",
    "time_taken = 10 + 2 * (software_program == 'B') + 3 * (software_program == 'C') + 1 * (experience_level == 'experienced') + np.random.normal(0, 1, n)\n",
    "\n",
    "data = pd.DataFrame({'SoftwareProgram': software_program, 'ExperienceLevel': experience_level, 'TimeTaken': time_taken})\n",
    "\n",
    "# two-way ANOVA\n",
    "formula = 'TimeTaken ~ C(SoftwareProgram) + C(ExperienceLevel) + C(SoftwareProgram):C(ExperienceLevel)'\n",
    "model = ols(formula, data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37c6fd-8992-4110-96e7-1adac1a79218",
   "metadata": {},
   "source": [
    "The ANOVA table will provide us with F-statistics, degrees of freedom, p-values, and other relevant information for the main effects and interaction effects of software program and experience level.\n",
    "The main effects of software program and experience level tell us if there are significant differences in time taken due to these factors individually.\n",
    "The interaction effect tells us if the effect of one factor depends on the level of the other factor.\n",
    "To interpret the results, we would examine the p-values associated with the main effects and interaction effect. A p-value below a chosen significance level (e.g., 0.05) indicates a statistically significant effect. If there is a significant interaction effect, it suggests that the effect of one factor on the dependent variable varies depending on the level of the other factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8742cb-33d9-4200-96f7-a1f91b02d2eb",
   "metadata": {},
   "source": [
    "### An educational researcher is interested in whether a new teaching method improves student testscores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "924d06b4-8675-4101-8f52-69e265b7c3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistic: -1.6677351961320235\n",
      "P-Value: 0.09856078338184605\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "np.random.seed(0)\n",
    "control_scores = np.random.normal(70, 10, 50)\n",
    "experimental_scores = np.random.normal(75, 10, 50)\n",
    "\n",
    "# two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2af5dba-ae1e-43e0-8642-53680c8e9763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Multiple Comparison of Means - Tukey HSD, FWER=0.05    \n",
      "==========================================================\n",
      " group1    group2    meandiff p-adj   lower  upper  reject\n",
      "----------------------------------------------------------\n",
      "Control Experimental    3.385 0.0986 -0.6429 7.4128  False\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# post-hoc test\n",
    "# DataFrame for the post-hoc test\n",
    "data = pd.DataFrame({'Scores': np.concatenate([control_scores, experimental_scores]),\n",
    "                     'Group': ['Control'] * len(control_scores) + ['Experimental'] * len(experimental_scores)})\n",
    "\n",
    "# Tukey-Kramer post-hoc test\n",
    "tukey_result = pairwise_tukeyhsd(data['Scores'], data['Group'])\n",
    "\n",
    "print(tukey_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48319fa-4381-47aa-89e5-7a8509188d32",
   "metadata": {},
   "source": [
    "### A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a posthoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f617685-2fc1-459a-b624-9bb54e11e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 0.600950051687737\n",
      "P-Value: 0.550550360677657\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(0)\n",
    "store_A_sales = np.random.normal(1000, 100, 30)\n",
    "store_B_sales = np.random.normal(1100, 120, 30)\n",
    "store_C_sales = np.random.normal(1050, 110, 30)\n",
    "\n",
    "# combine data \n",
    "sales_data = np.concatenate([store_A_sales, store_B_sales, store_C_sales])\n",
    "store_labels = ['A'] * 30 + ['B'] * 30 + ['C'] * 30\n",
    "\n",
    "df = pd.DataFrame({'Store': store_labels, 'Sales': sales_data})\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3155b-8eaa-4f7f-942c-3864cf240e59",
   "metadata": {},
   "source": [
    "The p-value (0.550550360677657) is greater than the commonly chosen significance level of 0.05. This suggests that there is not enough evidence to reject the null hypothesis. In other words, based on the data and the results of the analysis, it does not appear that there are significant differences in the average daily sales between the three retail stores (Store A, Store B, and Store C) for the 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4030067-ff98-4554-ad16-6a6baae98f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
